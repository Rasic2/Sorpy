### A set of examples for calculating the grad of tensor

- How to obtain grad of weights?

    ```
    x =  tensor([[-0.6341, 0.7444, -0.2436],
                [-0.7555, -0.6589, -0.1086],
                [-0.7707, -0.7338, -0.8581],
                [ 0.2972,  0.4482, -0.3238],
                [-0.7837, -0.7553, -0.5776]])
                
    w = tensor([[-0.4906, -0.9923, -0.3811],
                [-0.8488, -0.9170, -0.4238],
                [-0.6727, -0.6427, -0.4977]], requires_grad=True)

    y = matmul(x, w)

    y = tensor([[-0.1569,  0.1031,  0.0474],
                [ 1.0030,  1.4237,  0.6212],
                [ 1.5782,  1.9892,  1.0318],
                [-0.3085, -0.4979, -0.1421],
                [ 1.4142,  1.8416,  0.9063]], grad_fn=<MmBackward0>)

    target = tensor([[1., 1., 1.],
                    [1., 1., 1.],
                    [1., 1., 1.],
                    [1., 1., 1.],
                    [1., 1., 1.]])
  
    loss_fn = nn.L1Loss
  
    loss = tensor(0.7139, grad_fn=<L1LossBackward0>)
    ```
    result:
    
    ````
    loss.grad = 1
    y.grad = tensor([[-0.0667, -0.0667, -0.0667],
                     [ 0.0667,  0.0667, -0.0667],
                     [ 0.0667,  0.0667,  0.0667],
                     [-0.0667, -0.0667, -0.0667],
                     [ 0.0667,  0.0667, -0.0667]])
    y.grad[0][0] = (y[0][0]-target[0][0]) / abs(y[0][0]-target[0][0]) / 25

    w.grad = tensor([[-0.1315, -0.1315,  0.0737],
                     [-0.2227, -0.2227, -0.0342],
                     [-0.0651, -0.0651,  0.0264]])
    w.grad[0][0] = sum(x[:, 0] * y.grad[:, 0])
    ````
  
- How to obtain grad of biases?

    ````
    x = tensor([[-0.4467,  0.0921, -0.0556],
                [-0.3984, -0.7531, -0.1616],
                [-0.1773,  0.6360, -0.2101]])
  
    b = tensor([0.6540, 0.9255, 0.6607], requires_grad=True)
  
    y = x + b
    
    y = tensor([[0.2073, 1.0176, 0.6052],
                [0.2556, 0.1723, 0.4991],
                [0.4768, 1.5615, 0.4507]], grad_fn=<AddBackward0>)
    
    target = tensor([[1., 1., 1.],
                     [1., 1., 1.],
                     [1., 1., 1.]])
    
    loss_fn = nn.L1Loss
  
    loss = tensor(0.5458, grad_fn=<L1LossBackward0>)
    ````
    result:
    ````
    y.grad = tensor([[-0.1111,  0.1111, -0.1111],
                     [-0.1111, -0.1111, -0.1111],
                     [-0.1111,  0.1111, -0.1111]])
    
    b.grad = tensor([-0.3333,  0.1111, -0.3333])
    b.grad[0] = y.grad[:, 0]